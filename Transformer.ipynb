{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.6189, Valid Loss: 4.1112\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.0901, Valid Loss: 4.1139\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8785, Valid Loss: 3.8172\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.6092, Valid Loss: 3.4539\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3926, Valid Loss: 3.2839\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.2453, Valid Loss: 3.2201\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1528, Valid Loss: 3.2066\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1135, Valid Loss: 2.9919\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9702, Valid Loss: 2.9100\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8896, Valid Loss: 2.8794\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "def load_lexicon(lexicon_path):\n",
    "    # lexicon.txt格式假設為：\n",
    "    # word phone1 phone2 ...\n",
    "    # 如：ba b a\n",
    "    # a iNULL a\n",
    "    # 將每一行的第一個欄位當成拼音字串，其餘欄位是對應音素。\n",
    "    lex_map = {}\n",
    "    with open(lexicon_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            # 第一個為詞彙(如\"ba\")，後面為其對應音素序列\n",
    "            word = parts[0]\n",
    "            phones = parts[1:]\n",
    "            lex_map[word] = phones\n",
    "    return lex_map\n",
    "\n",
    "def text_to_phoneme_sequence(text, lex_map):\n",
    "    # text 如：\"li be e mih kiann lan lan san san long be tsiau tsng\"\n",
    "    # 一般是空白分詞，若需要根據您的實際資料格式調整\n",
    "    words = text.strip().split()\n",
    "    \n",
    "    phoneme_seq = []\n",
    "    for w in words:\n",
    "        # 檢查字典中是否有此詞\n",
    "        # 實務上，有些拼音可能要再細分或檢查\n",
    "        if w in lex_map:\n",
    "            phoneme_seq.extend(lex_map[w])\n",
    "        else:\n",
    "            # 若無對應，則可考慮略過或保留原文字串\n",
    "            # 這裡先簡單略過或將 w 作為一整個 token\n",
    "            # 實務上建議先確保 lexicon 完備\n",
    "            phoneme_seq.append(w)\n",
    "    return phoneme_seq\n",
    "\n",
    "def spec_augment(log_mel_spec, time_mask_num=1, freq_mask_num=1, time_mask_size=20, freq_mask_size=10):\n",
    "    # log_mel_spec: shape (n_mels, T)\n",
    "    n_mels, T = log_mel_spec.shape\n",
    "\n",
    "    # 頻率遮罩\n",
    "    for _ in range(freq_mask_num):\n",
    "        f = np.random.randint(0, freq_mask_size)\n",
    "        f_start = np.random.randint(0, n_mels - f)\n",
    "        log_mel_spec[f_start:f_start+f, :] = 0\n",
    "\n",
    "    # 時間遮罩\n",
    "    for _ in range(time_mask_num):\n",
    "        t = np.random.randint(0, time_mask_size)\n",
    "        t_start = np.random.randint(0, T - t)\n",
    "        log_mel_spec[:, t_start:t_start+t] = 0\n",
    "\n",
    "    return log_mel_spec\n",
    "\n",
    "def load_audio_features(wav_path, sr=16000, n_mels=80, frame_length=0.025, frame_shift=0.01):\n",
    "    # 載入 wav 音檔\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "    # 計算 n_fft, hop_length, win_length\n",
    "    n_fft = int(sr * frame_length)\n",
    "    hop_length = int(sr * frame_shift)\n",
    "    win_length = n_fft\n",
    "\n",
    "    # 計算 Mel-filterbank 特徵\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, \n",
    "                                              hop_length=hop_length, \n",
    "                                              win_length=win_length, \n",
    "                                              n_mels=n_mels, fmin=20, fmax=sr/2)\n",
    "    # 將能量取log(避免log(0)可加上小量)\n",
    "    log_mel_spec = np.log(np.maximum(mel_spec, 1e-10))\n",
    "    return log_mel_spec\n",
    "\n",
    "def preprocess_data(entries, wav_dir, lexicon_map):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    for row in entries:\n",
    "        utt_id = row['id']\n",
    "        text = row['text']\n",
    "        wav_path = os.path.join(wav_dir, utt_id + '.wav')\n",
    "        if not os.path.exists(wav_path):\n",
    "            print(f\"Warning: {wav_path} not found.\")\n",
    "            continue\n",
    "        feats = load_audio_features(wav_path)\n",
    "        phoneme_seq = text_to_phoneme_sequence(text, lexicon_map)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(phoneme_seq)\n",
    "    return features_list, labels_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lexicon_path = 'lexicon.txt'\n",
    "    train_csv_path = 'train-toneless.csv'\n",
    "    wav_dir = 'train_converted'\n",
    "    \n",
    "    # 讀取 lexicon\n",
    "    lex_map = load_lexicon(lexicon_path)\n",
    "    \n",
    "    # 讀取整個訓練 csv\n",
    "    entries = []\n",
    "    with open(train_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            entries.append(row)\n",
    "    \n",
    "    # 分割比例: 90% 用於訓練, 10% 用於驗證\n",
    "    train_entries, valid_entries = train_test_split(entries, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # 前處理訓練集資料\n",
    "    train_features, train_labels = preprocess_data(train_entries, wav_dir, lex_map)\n",
    "    # 前處理驗證集資料\n",
    "    valid_features, valid_labels = preprocess_data(valid_entries, wav_dir, lex_map)\n",
    "    \n",
    "    # 建立 phoneme->id 映射表 (根據訓練集和驗證集所有出現過的 phoneme)\n",
    "    all_phones = set(p for seq in train_labels+valid_labels for p in seq)\n",
    "    phone2id = {p: i for i, p in enumerate(sorted(all_phones))}\n",
    "    \n",
    "    # 將訓練與驗證標籤轉為 ID 序列\n",
    "    train_labels_id = [[phone2id[p] for p in seq] for seq in train_labels]\n",
    "    valid_labels_id = [[phone2id[p] for p in seq] for seq in valid_labels]\n",
    "    \n",
    "    # 至此您擁有：\n",
    "    # train_features, train_labels_id\n",
    "    # valid_features, valid_labels_id\n",
    "    # 可以進一步用於模型訓練及驗證。\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假設您已有以下資料 (由前處理階段取得)\n",
    "# train_features: List of np.array, each shape like (n_mels, T_frames)\n",
    "# train_labels_id: List of List[int]\n",
    "# valid_features: same structure as train_features\n",
    "# valid_labels_id: same structure as train_labels_id\n",
    "# phone2id: dict {phone_str: id_int}, 且有包含 <blank>:0\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, features_list, labels_list):\n",
    "        self.features_list = features_list\n",
    "        self.labels_list = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features_list[idx].T  # (T, n_mels)\n",
    "        label = self.labels_list[idx]\n",
    "        feat_tensor = torch.tensor(feat, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.int64)\n",
    "        return feat_tensor, label_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feat_lengths = [x[0].shape[0] for x in batch]\n",
    "    label_lengths = [x[1].shape[0] for x in batch]\n",
    "    max_feat_len = max(feat_lengths)\n",
    "    max_label_len = max(label_lengths)\n",
    "\n",
    "    n_mels = batch[0][0].shape[1]\n",
    "    feats_padded = torch.zeros(len(batch), max_feat_len, n_mels)\n",
    "    labels_padded = torch.zeros(len(batch), max_label_len, dtype=torch.int64)\n",
    "\n",
    "    for i, (f, l) in enumerate(batch):\n",
    "        feats_padded[i, :f.shape[0], :] = f\n",
    "        labels_padded[i, :l.shape[0]] = l\n",
    "\n",
    "    return feats_padded, labels_padded, torch.tensor(feat_lengths, dtype=torch.int64), torch.tensor(label_lengths, dtype=torch.int64)\n",
    "\n",
    "\n",
    "train_dataset = SpeechDataset(train_features, train_labels_id)\n",
    "valid_dataset = SpeechDataset(valid_features, valid_labels_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, T, d_model)\n",
    "        # 若 x 的長度小於 max_len 則可直接取前 T 長度的 encoding\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x\n",
    "\n",
    "class TransformerCTCModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=256, nhead=4, num_layers=2, dim_feedforward=1024, dropout=0.1):\n",
    "        super(TransformerCTCModel, self).__init__()\n",
    "        # 將 input_dim 映射到 d_model\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # 使用 TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 輸出層 (ctc 預測 phone id)\n",
    "        self.output_fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, T, input_dim)\n",
    "        # 先映射維度\n",
    "        x = self.input_fc(x)  # (batch, T, d_model)\n",
    "        # 加入位置編碼\n",
    "        x = self.pos_encoding(x)  # (batch, T, d_model)\n",
    "\n",
    "        # 對 x 做 transformer encoding\n",
    "        # 這裡假設沒有特殊的 mask，一般 CTC pipeline 不需特殊 mask\n",
    "        x = self.transformer_encoder(x)  # (batch, T, d_model)\n",
    "\n",
    "        # 輸出 logits\n",
    "        logits = self.output_fc(x) # (batch, T, output_dim)\n",
    "\n",
    "        # CTC loss 預設要求 (T, batch, class)\n",
    "        logits = logits.permute(1, 0, 2) # (T, batch, output_dim)\n",
    "        return logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_mels = train_features[0].shape[0]\n",
    "output_dim = len(phone2id)\n",
    "hidden_dim = 256\n",
    "num_layers = 3\n",
    "\n",
    "model = TransformerCTCModel(input_dim=n_mels, output_dim=output_dim, d_model=128, nhead=2, num_layers=2).to(device)\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for feats, labels, feat_lens, label_lens in pbar:\n",
    "        feats = feats.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats) # (T, batch, output_dim)\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        \n",
    "        loss = criterion(log_probs, labels, feat_lens, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = feats.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for feats, labels, feat_lens, label_lens in pbar:\n",
    "            feats = feats.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(feats)\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            loss = criterion(log_probs, labels, feat_lens, label_lens)\n",
    "\n",
    "            batch_size = feats.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            pbar.set_postfix(val_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "# Early Stopping 機制\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, save_path=\"Transformer_model.pth\"):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.save_path = save_path\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), self.save_path)\n",
    "\n",
    "num_epochs = 10\n",
    "early_stopping = EarlyStopping(patience=50, delta=0.0, save_path=\"Transformer_model.pth\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    valid_loss = validate(model, valid_loader, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping 判斷\n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# 訓練完成後，已經透過 EarlyStopping 儲存最佳模型於 \"Transformer_model.pth\"\n",
    "# 若想儲存最後的模型狀態，也可另外 torch.save(model.state_dict(), \"final_model.pth\")\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# 假設已經有以下變數:\n",
    "# model: 訓練完成後的 RNNCTCModel，且已載入權重\n",
    "# device: torch.device\n",
    "# phone2id: dict {phone_str: id_int}, 並包含 <blank>:0\n",
    "id2phone = {v: k for k, v in phone2id.items()}\n",
    "\n",
    "def load_audio_features(wav_path, sr=16000, n_mels=80, frame_length=0.025, frame_shift=0.01):\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "    n_fft = int(sr * frame_length)\n",
    "    hop_length = int(sr * frame_shift)\n",
    "    win_length = n_fft\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,\n",
    "                                              hop_length=hop_length,\n",
    "                                              win_length=win_length,\n",
    "                                              n_mels=n_mels, fmin=20, fmax=sr/2)\n",
    "    log_mel_spec = np.log(np.maximum(mel_spec, 1e-10))\n",
    "    return log_mel_spec\n",
    "\n",
    "def ctc_greedy_decode(logits, blank_id=0):\n",
    "    # logits: (T, batch, output_dim)\n",
    "    # greedy decode\n",
    "    pred_ids = torch.argmax(logits, dim=-1)  # (T, batch)\n",
    "    pred_ids = pred_ids.transpose(0,1)       # (batch, T)\n",
    "    results = []\n",
    "    for seq in pred_ids:\n",
    "        prev = blank_id\n",
    "        out = []\n",
    "        for p in seq:\n",
    "            p = p.item()\n",
    "            if p != blank_id and p != prev:\n",
    "                out.append(p)\n",
    "            prev = p\n",
    "        results.append(out)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# 假設已經有以下變數:\n",
    "# model: 訓練完成後的 RNNCTCModel，且已載入權重\n",
    "# device: torch.device\n",
    "# phone2id: dict {phone_str: id_int}, 並包含 <blank>:0\n",
    "id2phone = {v: k for k, v in phone2id.items()}\n",
    "\n",
    "def load_audio_features(wav_path, sr=16000, n_mels=80, frame_length=0.025, frame_shift=0.01):\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "    n_fft = int(sr * frame_length)\n",
    "    hop_length = int(sr * frame_shift)\n",
    "    win_length = n_fft\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,\n",
    "                                              hop_length=hop_length,\n",
    "                                              win_length=win_length,\n",
    "                                              n_mels=n_mels, fmin=20, fmax=sr/2)\n",
    "    log_mel_spec = np.log(np.maximum(mel_spec, 1e-10))\n",
    "    return log_mel_spec\n",
    "\n",
    "def ctc_greedy_decode(logits, blank_id=0):\n",
    "    # logits: (T, batch, output_dim)\n",
    "    # greedy decode\n",
    "    pred_ids = torch.argmax(logits, dim=-1)  # (T, batch)\n",
    "    pred_ids = pred_ids.transpose(0,1)       # (batch, T)\n",
    "    results = []\n",
    "    for seq in pred_ids:\n",
    "        prev = blank_id\n",
    "        out = []\n",
    "        for p in seq:\n",
    "            p = p.item()\n",
    "            if p != blank_id and p != prev:\n",
    "                out.append(p)\n",
    "            prev = p\n",
    "        results.append(out)\n",
    "    return results\n",
    "\n",
    "def inference(model, test_wav_dir, sample_csv_path, output_csv_path):\n",
    "    # sample.csv: 欄位 (id, text)\n",
    "    # 我們會根據 sample.csv 的 id 讀取 test_converted 目錄下的同名音檔\n",
    "    # 並將推論結果寫回 output_csv_path\n",
    "    test_entries = []\n",
    "    with open(sample_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            test_entries.append(row)\n",
    "\n",
    "    model.eval()\n",
    "    n_mels = train_features[0].shape[0]  # 假設與訓練時相同的 n_mels\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for entry in test_entries:\n",
    "            utt_id = entry[\"id\"]\n",
    "            \n",
    "            wav_path = os.path.join(test_wav_dir, utt_id + \".wav\")\n",
    "            if not os.path.exists(wav_path):\n",
    "                print(f\"Warning: {wav_path} not found.\")\n",
    "                # 若音檔不存在，可用空結果或略過\n",
    "                predictions.append({\"id\": utt_id, \"text\": \"\"})\n",
    "                continue\n",
    "\n",
    "            feats = load_audio_features(wav_path)\n",
    "            feats_t = torch.tensor(feats.T, dtype=torch.float32).unsqueeze(0).to(device) # (1, T, n_mels)\n",
    "\n",
    "            logits = model(feats_t)  # (T, batch, output_dim)\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            pred_ids = ctc_greedy_decode(log_probs, blank_id=0)[0]\n",
    "\n",
    "            pred_phones = [id2phone[i] for i in pred_ids if id2phone[i] != \"iNULL\"]\n",
    "            pred_text = \" \".join(pred_phones)\n",
    "\n",
    "            predictions.append({\"id\": utt_id, \"text\": pred_text})\n",
    "\n",
    "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"text\"])\n",
    "        writer.writeheader()\n",
    "        for pred in predictions:\n",
    "            writer.writerow(pred)\n",
    "\n",
    "# 假設此時您已載入模型最佳參數:\n",
    "model.load_state_dict(torch.load(\"Transformer_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 執行推論(不計算Levenshtein Distance)\n",
    "inference(model, \"test_converted\", \"sample.csv\", \"sample_Transformer.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
