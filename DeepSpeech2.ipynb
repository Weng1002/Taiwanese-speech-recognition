{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa pandas torch torchaudio transformers tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def convert_audio(input_dir, output_dir):\n",
    "    # 確保輸出目錄存在\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 遍歷目錄中的所有 .wav 文件\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                input_path = os.path.join(root, file)\n",
    "                \n",
    "                # 生成輸出文件的路徑\n",
    "                relative_path = os.path.relpath(root, input_dir)  # 保留相對路徑\n",
    "                output_folder = os.path.join(output_dir, relative_path)\n",
    "                if not os.path.exists(output_folder):\n",
    "                    os.makedirs(output_folder)\n",
    "                \n",
    "                output_path = os.path.join(output_folder, file)\n",
    "                \n",
    "                # 調用 Sox 進行格式轉換\n",
    "                command = [\n",
    "                    \"sox\", input_path,\n",
    "                    \"-r\", \"16000\",         # 16 kHz\n",
    "                    \"-e\", \"signed-integer\",# signed-integer\n",
    "                    \"-b\", \"16\",            # 16 bits\n",
    "                    output_path\n",
    "                ]\n",
    "                \n",
    "                print(f\"Converting: {input_path} -> {output_path}\")\n",
    "                subprocess.run(command, check=True)\n",
    "\n",
    "# 設置資料夾\n",
    "train_input_dir = \"train\"\n",
    "train_output_dir = \"train_converted\"\n",
    "test_input_dir = \"test\"\n",
    "test_output_dir = \"test_converted\"\n",
    "\n",
    "# 轉換 train 和 test 資料夾中的音頻\n",
    "convert_audio(train_input_dir, train_output_dir)\n",
    "convert_audio(test_input_dir, test_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_lexicon(lexicon_path):\n",
    "    # lexicon.txt格式假設為：\n",
    "    # word phone1 phone2 ...\n",
    "    # 如：ba b a\n",
    "    # a iNULL a\n",
    "    # 將每一行的第一個欄位當成拼音字串，其餘欄位是對應音素。\n",
    "    lex_map = {}\n",
    "    with open(lexicon_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            # 第一個為詞彙(如\"ba\")，後面為其對應音素序列\n",
    "            word = parts[0]\n",
    "            phones = parts[1:]\n",
    "            lex_map[word] = phones\n",
    "    return lex_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_phoneme_sequence(text, lex_map):\n",
    "    # text 如：\"li be e mih kiann lan lan san san long be tsiau tsng\"\n",
    "    # 一般是空白分詞，若需要根據您的實際資料格式調整\n",
    "    words = text.strip().split()\n",
    "    \n",
    "    phoneme_seq = []\n",
    "    for w in words:\n",
    "        # 檢查字典中是否有此詞\n",
    "        # 實務上，有些拼音可能要再細分或檢查\n",
    "        if w in lex_map:\n",
    "            phoneme_seq.extend(lex_map[w])\n",
    "        else:\n",
    "            # 若無對應，則可考慮略過或保留原文字串\n",
    "            # 這裡先簡單略過或將 w 作為一整個 token\n",
    "            # 實務上建議先確保 lexicon 完備\n",
    "            phoneme_seq.append(w)\n",
    "    return phoneme_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(log_mel_spec, time_mask_num=1, freq_mask_num=1, time_mask_size=20, freq_mask_size=10):\n",
    "    # log_mel_spec: shape (n_mels, T)\n",
    "    n_mels, T = log_mel_spec.shape\n",
    "\n",
    "    # 頻率遮罩\n",
    "    for _ in range(freq_mask_num):\n",
    "        f = np.random.randint(0, freq_mask_size)\n",
    "        f_start = np.random.randint(0, n_mels - f)\n",
    "        log_mel_spec[f_start:f_start+f, :] = 0\n",
    "\n",
    "    # 時間遮罩\n",
    "    for _ in range(time_mask_num):\n",
    "        t = np.random.randint(0, time_mask_size)\n",
    "        t_start = np.random.randint(0, T - t)\n",
    "        log_mel_spec[:, t_start:t_start+t] = 0\n",
    "\n",
    "    return log_mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_features(wav_path, sr=16000, n_mels=80, frame_length=0.025, frame_shift=0.01):\n",
    "    # 載入 wav 音檔\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "    # 計算 n_fft, hop_length, win_length\n",
    "    n_fft = int(sr * frame_length)\n",
    "    hop_length = int(sr * frame_shift)\n",
    "    win_length = n_fft\n",
    "\n",
    "    # 計算 Mel-filterbank 特徵\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, \n",
    "                                              hop_length=hop_length, \n",
    "                                              win_length=win_length, \n",
    "                                              n_mels=n_mels, fmin=20, fmax=sr/2)\n",
    "    # 將能量取log(避免log(0)可加上小量)\n",
    "    log_mel_spec = np.log(np.maximum(mel_spec, 1e-10))\n",
    "    return log_mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(entries, wav_dir, lexicon_map):\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    for row in entries:\n",
    "        utt_id = row['id']\n",
    "        text = row['text']\n",
    "        wav_path = os.path.join(wav_dir, utt_id + '.wav')\n",
    "        if not os.path.exists(wav_path):\n",
    "            print(f\"Warning: {wav_path} not found.\")\n",
    "            continue\n",
    "        feats = load_audio_features(wav_path)\n",
    "        phoneme_seq = text_to_phoneme_sequence(text, lexicon_map)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(phoneme_seq)\n",
    "    return features_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lexicon_path = 'lexicon.txt'\n",
    "    train_csv_path = 'train-toneless.csv'\n",
    "    wav_dir = 'train_converted'\n",
    "    \n",
    "    # 讀取 lexicon\n",
    "    lex_map = load_lexicon(lexicon_path)\n",
    "    \n",
    "    # 讀取整個訓練 csv\n",
    "    entries = []\n",
    "    with open(train_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            entries.append(row)\n",
    "    \n",
    "    # 分割比例: 90% 用於訓練, 10% 用於驗證\n",
    "    train_entries, valid_entries = train_test_split(entries, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # 前處理訓練集資料\n",
    "    train_features, train_labels = preprocess_data(train_entries, wav_dir, lex_map)\n",
    "    # 前處理驗證集資料\n",
    "    valid_features, valid_labels = preprocess_data(valid_entries, wav_dir, lex_map)\n",
    "    \n",
    "    # 建立 phoneme->id 映射表 (根據訓練集和驗證集所有出現過的 phoneme)\n",
    "    all_phones = set(p for seq in train_labels+valid_labels for p in seq)\n",
    "    phone2id = {p: i for i, p in enumerate(sorted(all_phones))}\n",
    "    \n",
    "    # 將訓練與驗證標籤轉為 ID 序列\n",
    "    train_labels_id = [[phone2id[p] for p in seq] for seq in train_labels]\n",
    "    valid_labels_id = [[phone2id[p] for p in seq] for seq in valid_labels]\n",
    "    \n",
    "    # 至此您擁有：\n",
    "    # train_features, train_labels_id\n",
    "    # valid_features, valid_labels_id\n",
    "    # 可以進一步用於模型訓練及驗證。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 假設您已經由前處理程式碼取得:\n",
    "# train_features, train_labels_id, valid_features, valid_labels_id, phone2id\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return feat, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # 與前面範例類似的collate，將不同長度序列對齊\n",
    "    max_time = max(f.shape[1] for f, _ in batch)\n",
    "    max_label_len = max(len(l) for _, l in batch)\n",
    "    \n",
    "    n_mels = batch[0][0].shape[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    feats_padded = np.zeros((batch_size, n_mels, max_time), dtype=np.float32)\n",
    "    labels_padded = np.zeros((batch_size, max_label_len), dtype=np.int32)\n",
    "    input_lengths = []\n",
    "    target_lengths = []\n",
    "\n",
    "    for i, (f, l) in enumerate(batch):\n",
    "        T = f.shape[1]\n",
    "        L = len(l)\n",
    "        feats_padded[i, :, :T] = f\n",
    "        labels_padded[i, :L] = l\n",
    "        input_lengths.append(T)\n",
    "        target_lengths.append(L)\n",
    "\n",
    "    feats_padded = torch.from_numpy(feats_padded).unsqueeze(1) # (B, 1, n_mels, T)\n",
    "    # DeepSpeech2 通常使用 (B, C, F, T) 格式的輸入，C=1為單通道\n",
    "    # 不過這裡n_mels在F維、T在W維的話，需要注意conv層的kernel維度設計\n",
    "    # 我們假設Conv的輸入 (B, C, F, T) 中 F對應頻率, T對應時間\n",
    "    # 已有 feats_padded shape 為 (B, n_mels, T) -> (B,1,n_mels,T)，\n",
    "    # 我們將 n_mels 視為頻率維度， T 視為時間維度\n",
    "\n",
    "    labels_padded = torch.from_numpy(labels_padded)\n",
    "    input_lengths = torch.tensor(input_lengths, dtype=torch.int32)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.int32)\n",
    "\n",
    "    return feats_padded, labels_padded, input_lengths, target_lengths\n",
    "\n",
    "train_dataset = SpeechDataset(train_features, train_labels_id)\n",
    "valid_dataset = SpeechDataset(valid_features, valid_labels_id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class DeepSpeech2(nn.Module):\n",
    "    def __init__(self, input_freq_dim, num_classes, rnn_hidden_size=512, rnn_layers=5, bidirectional=True):\n",
    "        super(DeepSpeech2, self).__init__()\n",
    "        \n",
    "        # 前端卷積層：假設使用2層2D卷積，如論文中所示\n",
    "        # 論文中使用 kernel size= (41,11), stride=(2,2)等較大的kernel進行時間/頻率子採樣\n",
    "        # 這裡簡化使用較小的kernel，例如 (3,3)，根據需求可調整\n",
    "        # Input shape: (B, 1, n_mels, T)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), stride=(2,2), padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # 經過兩層conv後，時間與頻率維度都會縮小，需計算縮小後的維度\n",
    "        # 假設 n_mels為F, T為時間維度\n",
    "        # 兩層conv各有stride=2, 則 F維度和T維度各縮小4倍 (2層,每層2倍)\n",
    "        # 新的頻率維度: ceil(F/2) -> 再/2\n",
    "        # 新的時間維度：同上\n",
    "        # 這裡計算一下最終的F', T'\n",
    "        # 簡化處理：若 F與T足夠大，我們可直接forward一次dummy tensor來求輸出維度\n",
    "        \n",
    "        # 先dummy forward計算RNN輸入維度\n",
    "        dummy_input = torch.zeros(1, 1, input_freq_dim, 200) # 假設T=200作測試\n",
    "        with torch.no_grad():\n",
    "            conv_out = self.conv(dummy_input)\n",
    "        _, c_out, f_out, t_out = conv_out.shape\n",
    "        # conv_out shape: (1, c_out, f_out, t_out)\n",
    "        # RNN輸入需要 (B, T, Feature)，我們需將C,F,T展平為 T和Feature\n",
    "        # 具體而言，我們會將 freq和channel合併，最後RNN的input_dim = c_out * f_out\n",
    "        \n",
    "        rnn_input_dim = c_out * f_out\n",
    "\n",
    "        # RNN部分 (使用GRU)\n",
    "        # 論文用的為GRU，這裡採用GRU，layers=5為例\n",
    "        self.rnn = nn.GRU(input_size=rnn_input_dim, hidden_size=rnn_hidden_size, \n",
    "                          num_layers=rnn_layers, batch_first=True, \n",
    "                          bidirectional=bidirectional)\n",
    "\n",
    "        factor = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(rnn_hidden_size * factor, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, F, T)\n",
    "        x = self.conv(x)  # (B, c_out, f_out, t_out)\n",
    "        \n",
    "        # 將輸出轉為 (B, T_out, c_out*f_out)\n",
    "        B, C, F, T = x.size()\n",
    "        # 轉換為 (B, T, C*F)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous() # (B, T, C, F)\n",
    "        x = x.view(B, T, C*F)\n",
    "\n",
    "        # 經過RNN\n",
    "        x, _ = self.rnn(x) # (B, T, hidden*factor)\n",
    "\n",
    "        # 全連接層映射至 num_classes\n",
    "        logits = self.fc(x) # (B, T, num_classes)\n",
    "\n",
    "        # CTC需要log_softmax\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "        return log_probs\n",
    "\n",
    "n_mels = train_features[0].shape[0]\n",
    "num_phones = len(phone2id)\n",
    "blank_id = 0  # 假設blank是0\n",
    "\n",
    "model = DeepSpeech2(input_freq_dim=n_mels, num_classes=num_phones, rnn_hidden_size=512, rnn_layers=5, bidirectional=True)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=blank_id, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for feats_padded, labels_padded, input_lengths, target_lengths in train_loader:\n",
    "        # feats_padded: (B, 1, n_mels, T)\n",
    "        feats_padded = feats_padded.cuda()\n",
    "        labels_padded = labels_padded.cuda()\n",
    "        input_lengths = input_lengths.cuda()\n",
    "        target_lengths = target_lengths.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(feats_padded) # (B, T', num_classes)\n",
    "        # 計算CTC的時候，要注意卷積後時間軸長度改變了，\n",
    "        # 因此input_lengths也要依據卷積縮小倍率做相應的縮減。\n",
    "        # 假設縮短了4倍時間(因為stride=2兩次)，新的input_lengths = old_lengths // 4 或其它計算(需根據實際stride與padding計算)\n",
    "        \n",
    "        # 如果您固定了卷積的stride與padding，可以在collate_fn中提前計算縮短後的input_lengths並一併回傳\n",
    "        # 這裡以簡化處理：假設每次都固定一樣的縮放比例 (stride=2兩層 => 總共時間縮小4倍)\n",
    "        new_input_lengths = (input_lengths // 4).to(torch.int32)\n",
    "        \n",
    "        # log_probs shape for CTC: (T, B, C)\n",
    "        log_probs = log_probs.transpose(0, 1)\n",
    "        loss = criterion(log_probs, labels_padded, new_input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # 驗證\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for feats_padded, labels_padded, input_lengths, target_lengths in valid_loader:\n",
    "            feats_padded = feats_padded.cuda()\n",
    "            labels_padded = labels_padded.cuda()\n",
    "            input_lengths = input_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "\n",
    "            log_probs = model(feats_padded)\n",
    "            new_input_lengths = (input_lengths // 4).to(torch.int32)\n",
    "            log_probs = log_probs.transpose(0, 1)\n",
    "            loss = criterion(log_probs, labels_padded, new_input_lengths, target_lengths)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "    \n",
    "# 在訓練完成後儲存模型權重\n",
    "torch.save(model.state_dict(), \"deepspeech2_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# 假設您已有:\n",
    "# model: DeepSpeech2 模型並已load訓練後參數\n",
    "# device: torch.device\n",
    "# phone2id: {phone_str: id_int}, 並包含 <blank>:0\n",
    "id2phone = {v: k for k, v in phone2id.items()}\n",
    "\n",
    "def load_audio_features(wav_path, sr=16000, n_mels=80, frame_length=0.025, frame_shift=0.01):\n",
    "    y, sr = librosa.load(wav_path, sr=sr)\n",
    "    n_fft = int(sr * frame_length)\n",
    "    hop_length = int(sr * frame_shift)\n",
    "    win_length = n_fft\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,\n",
    "                                              hop_length=hop_length,\n",
    "                                              win_length=win_length,\n",
    "                                              n_mels=n_mels, fmin=20, fmax=sr/2)\n",
    "    log_mel_spec = np.log(np.maximum(mel_spec, 1e-10))\n",
    "    return log_mel_spec\n",
    "\n",
    "def ctc_greedy_decode(logits, blank_id=0):\n",
    "    # logits: (T, batch, output_dim)\n",
    "    pred_ids = torch.argmax(logits, dim=-1)  # (T, batch)\n",
    "    pred_ids = pred_ids.transpose(0,1)       # (batch, T)\n",
    "    results = []\n",
    "    for seq in pred_ids:\n",
    "        prev = blank_id\n",
    "        out = []\n",
    "        for p in seq:\n",
    "            p = p.item()\n",
    "            if p != blank_id and p != prev:\n",
    "                out.append(p)\n",
    "            prev = p\n",
    "        results.append(out)\n",
    "    return results\n",
    "\n",
    "def inference(model, test_wav_dir, sample_csv_path, output_csv_path):\n",
    "    test_entries = []\n",
    "    with open(sample_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            test_entries.append(row)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for entry in test_entries:\n",
    "            utt_id = entry[\"id\"]\n",
    "            wav_path = os.path.join(test_wav_dir, utt_id + \".wav\")\n",
    "            if not os.path.exists(wav_path):\n",
    "                print(f\"Warning: {wav_path} not found.\")\n",
    "                predictions.append({\"id\": utt_id, \"text\": \"\"})\n",
    "                continue\n",
    "\n",
    "            feats = load_audio_features(wav_path)  # (n_mels, T)\n",
    "            # DeepSpeech2 輸入維度 (B,1,F,T)\n",
    "            # feats shape為 (n_mels,T) -> 增加batch與channel維度 (1,1,n_mels,T)\n",
    "            feats_t = torch.tensor(feats, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            # 模型輸出為 (B,T',C)\n",
    "            logits = model(feats_t)  # (B, T', C)\n",
    "            # 取log softmax (model若已回傳log_probs則可省略)\n",
    "            # 這裡假設model已回傳log_probs，如模型中已有log_softmax則可直接使用logits\n",
    "            # 若model回傳的是logits，請使用:\n",
    "            # log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "            log_probs = logits  # 若上面forward已經是log_softmax的結果\n",
    "\n",
    "            # CTC decode預期 (T',B,C)\n",
    "            log_probs = log_probs.transpose(0,1)  # (T',B,C)\n",
    "            pred_ids = ctc_greedy_decode(log_probs, blank_id=0)[0]\n",
    "\n",
    "            pred_phones = [id2phone[i] for i in pred_ids if id2phone[i] != \"iNULL\"]\n",
    "            pred_text = \" \".join(pred_phones)\n",
    "            predictions.append({\"id\": utt_id, \"text\": pred_text})\n",
    "\n",
    "    with open(output_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"id\", \"text\"])\n",
    "        writer.writeheader()\n",
    "        for pred in predictions:\n",
    "            writer.writerow(pred)\n",
    "\n",
    "# 載入模型參數\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"deepspeech2_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 推論\n",
    "inference(model, \"test_converted\", \"sample.csv\", \"sample_DeepSpeech2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
